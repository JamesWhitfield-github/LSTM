{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAMESWhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading the data and building the different classes of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_WDS_results(filepath):\n",
    "    \n",
    "    # Read in data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Read in data\n",
    "    df_original = pd.read_csv(filepath)\n",
    "    \n",
    "    # Fill in NaN with the mean\n",
    "    #df.loc[:, df.columns != 'Skill Level'] = df.loc[:, df.columns != 'Skill Level'].fillna(df.mean())\n",
    "    \n",
    "    # Generate total engagement score\n",
    "    if 'Engagement Minimum Data Score' in df.columns:\n",
    "        df['Engagement Data Score'] = df['Engagement Minimum Data Score'].fillna(0) + \\\n",
    "        (df['Engagement Low Data Score'].fillna(0)*1) + \\\n",
    "        (df['Engagement Medium Data Score'].fillna(0)*3) + \\\n",
    "        (df['Engagement High Data Score'].fillna(0)*6)\n",
    "\n",
    "        # Fill in NaN where no engagement scores are provided\n",
    "        df.loc[pd.isna(df['Engagement Minimum Data Score']) & \n",
    "                pd.isna(df['Engagement Low Data Score']) &\n",
    "                pd.isna(df['Engagement Medium Data Score']) &\n",
    "                pd.isna(df['Engagement High Data Score']), 'Engagement Data Score'] = None\n",
    "    else:\n",
    "        df = df\n",
    "    \n",
    "    # Fill in NaN with the mean\n",
    "    df.loc[:, ['Activity Data Score',\n",
    "               'Feedback Data Score',\n",
    "              'Checklist Data Score',\n",
    "              'Competency Beginning Data Score',\n",
    "              'Competency Progressing Data Score',\n",
    "              'Competency Proficient Data Score',\n",
    "              'Competency Mastery Data Score',\n",
    "              'AboutMe Data Score',\n",
    "              'CVs Data Score']] = df.loc[:, ['Activity Data Score',\n",
    "               'Feedback Data Score',\n",
    "              'Checklist Data Score',\n",
    "              'Competency Beginning Data Score',\n",
    "              'Competency Progressing Data Score',\n",
    "              'Competency Proficient Data Score',\n",
    "              'Competency Mastery Data Score',\n",
    "              'AboutMe Data Score',\n",
    "              'CVs Data Score']].fillna(df.mean())\n",
    "    \n",
    "    # Fill in NaN with the 0\n",
    "    df.loc[:, ['Achievement Data Score',\n",
    "               'Courses Data Score',\n",
    "               'Engagement Minimum Data Score',\n",
    "              'Engagement Low Data Score',\n",
    "              'Engagement Medium Data Score',\n",
    "              'Engagement High Data Score',\n",
    "              'Engagement Data Score',\n",
    "              'BadgeData Data Score',\n",
    "              'Learning Course Data Score',\n",
    "              'KCs Data Score']] = df.loc[:, ['Achievement Data Score',\n",
    "               'Courses Data Score',\n",
    "               'Engagement Minimum Data Score',\n",
    "              'Engagement Low Data Score',\n",
    "              'Engagement Medium Data Score',\n",
    "              'Engagement High Data Score',\n",
    "              'Engagement Data Score',\n",
    "              'BadgeData Data Score',\n",
    "              'Learning Course Data Score',\n",
    "              'KCs Data Score']].fillna(0)\n",
    "    \n",
    "    # Generate total competency score\n",
    "    if 'Competency Beginning Data Score' in df.columns:\n",
    "        df['Competency Data Score'] = df['Competency Beginning Data Score'].fillna(0) + \\\n",
    "        (df['Competency Progressing Data Score'].fillna(0)*2) + \\\n",
    "        (df['Competency Proficient Data Score'].fillna(0)*4) + \\\n",
    "        (df['Competency Mastery Data Score'].fillna(0)*8)\n",
    "\n",
    "        # Fill in NaN where no competencies are provided\n",
    "        df.loc[pd.isna(df['Competency Beginning Data Score']) & \n",
    "                pd.isna(df['Competency Progressing Data Score']) &\n",
    "                pd.isna(df['Competency Proficient Data Score']) &\n",
    "                pd.isna(df['Competency Mastery Data Score']), 'Competency Data Score'] = None\n",
    "    else:\n",
    "        df = df\n",
    "        \n",
    "    \n",
    "    # Reset the index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Reset the index\n",
    "    df_original = df_original.reset_index()\n",
    "    \n",
    "    return df, df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_model(df, feature_cols):\n",
    "    \n",
    "    # Drop the NaN in the training data\n",
    "    df = df.dropna(subset=['Skill Level']).copy()\n",
    "    \n",
    "    # Split dataset in features and target variable\n",
    "    X = df[feature_cols] # Features\n",
    "    y = df['Skill Level'] # Target variable\n",
    "    \n",
    "    # Split dataset into training and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.0,random_state=7)\n",
    "    \n",
    "    # Instantiate the model (using the default parameters)\n",
    "    logreg = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "    \n",
    "    # Define feature selection\n",
    "    selector = RFE(logreg, step=1)\n",
    "\n",
    "    # Fit the model with data\n",
    "    selector.fit(X_train,y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    #y_pred = selector.predict(X_test)\n",
    "    #y_pred = selector.predict(X_train)\n",
    "    #y_pred = pd.DataFrame(y_pred)\n",
    "    #y_pred = y_pred.rename(index=str, columns={0: \"Predictions\"})\n",
    "    \n",
    "    # Confidence\n",
    "    #confidence = selector.predict_proba(X_test)\n",
    "    #confidence = selector.predict_proba(X_train)\n",
    "    #confidence = confidence.max(axis=1)\n",
    "    #confidence_df = pd.DataFrame(confidence)\n",
    "    #confidence_df = confidence_df.rename(index=str, columns={0: \"Confidence\"})\n",
    "    #y_pred = pd.merge(y_pred, confidence_df, left_index=True, right_index=True)\n",
    "    \n",
    "    # Convert y datasets to pandas dataframes\n",
    "    #y_train = pd.DataFrame(data=y_train)\n",
    "    #y_test = pd.DataFrame(data=y_test)\n",
    "    \n",
    "    # Set index to match y_test\n",
    "    #index_vals = y_test.index.values\n",
    "    #index_vals = y_train.index.values\n",
    "    #y_pred = y_pred.set_index(index_vals)\n",
    "    \n",
    "    #selections = selector.support_\n",
    "    \n",
    "    return selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_keyword_results(filepath,sheet):\n",
    "    \n",
    "    # Read in data\n",
    "    df = pd.read_excel(io=filepath, sheet_name=sheet)\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    df = df.drop(columns=['Activity Data Score', 'Achievement Data Score','Courses Data Score',\n",
    "                          'Engagement Minimum Data Score','Engagement Low Data Score',\n",
    "                          'Engagement Medium Data Score','Engagement High Data Score',\n",
    "                          'Feedback Data Score','BadgeData Data Score','Competency Beginning Data Score', \n",
    "                          'Competency Progressing Data Score','Competency Proficient Data Score',\n",
    "                          'Competency Mastery Data Score','Learning Course Data Score','AboutMe Data Score',\n",
    "                          'CVs Data Score','Checklist Data Score','KCs Data Score'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_mapping = {\n",
    "    'Purchase Price Allocation (PPA)': (1, 'Purchase Price Allocation (PPA)'),\n",
    "    'Valuation': (2, 'Valuation'),\n",
    "    'Impairment Testing': (3, 'Impairment Testing'),\n",
    "    'Equity Incentives - Tax Valuati': (4, 'Equity Incentives - Tax Valuation'),\n",
    "    'Cost Benefit Analysis': (5, 'Cost Benefit Analysis'),\n",
    "    'Due Diligence': (6, 'Due Diligence'),\n",
    "    'Real estate sell-side preparati': (7, 'Real estate sell-side preparation'),\n",
    "    'Modelling': (8, 'Modelling'),\n",
    "    'Debt Options Analysis': (9, 'Debt Options Analysis'),\n",
    "    'Bid financial evaluation': (10, 'Bid financial evaluation'),\n",
    "    'Green Book business cases': (11, 'Green Book business cases'),\n",
    "    'Power Purchase Agreements (PPA': (12, 'Power Purchase Agreements (PPAs)'),\n",
    "    'Working Capital & Liquidity Imp': (13, 'Working Capital & Liquidity Improvement'),\n",
    "    'Financial Analysis Due Diligenc': (14, 'Financial Analysis Due Diligence'),\n",
    "    'Advanced Excel': (15, 'Advanced Excel'),\n",
    "    'Cash flow Forecasting': (16, 'Cash flow Forecasting'),\n",
    "    'Formal Insolvency': (17, 'Formal Insolvency'),\n",
    "    'Equity to Enterprise Value Brid': (18, 'Equity to Enterprise Value Bridges'),\n",
    "    'EBITDA Adjustment Identificatio': (19, 'EBITDA Adjustment Identification'),\n",
    "    'Debt and debt-like items analys': (20, 'Debt and debt-like items analysis'),\n",
    "    'Stock Exchange Working Capital': (21, 'Stock Exchange Working Capital'),\n",
    "    'Working capital target /peg/ normalised analysis': (22, 'Working capital target peg norm'),\n",
    "    'Project Management': (23, 'Project Management')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(filepath_WDS,skill,feature_cols,filepath_keyword,skill_mapping):\n",
    "    \n",
    "    # Read in data\n",
    "    df, df_original = import_WDS_results(filepath_WDS)\n",
    "    df = df.drop(['index'], axis=1)\n",
    "    df_original = df_original.drop(['index'], axis=1)\n",
    "    \n",
    "    # Build the model\n",
    "    selector = logistic_regression_model(df, feature_cols)\n",
    "\n",
    "    # Create table of test predictions\n",
    "    #predictions_test = pd.merge(df, y_pred, left_index=True, right_index=True)\n",
    "    #predictions_test['Test Flag'] = 1\n",
    "    #predictions_test['Train Flag'] = 1\n",
    "    #predictions_test\n",
    "    \n",
    "    # Create table of predictions\n",
    "    X = df[feature_cols]\n",
    "    y_pred = pd.DataFrame(selector.predict(X))\n",
    "    y_pred = y_pred.rename(index=int, columns={0: \"Predictions\"})\n",
    "    \n",
    "    # Confidence\n",
    "    confidence = selector.predict_proba(X)\n",
    "    max_confidence = confidence.max(axis=1)\n",
    "    max_confidence_df = pd.DataFrame(max_confidence)\n",
    "    max_confidence_df = max_confidence_df.rename(index=int, columns={0: \"Confidence\"})\n",
    "    y_pred = pd.merge(y_pred, max_confidence_df, left_index=True, right_index=True)\n",
    "    \n",
    "    # Reset Index of y_pred\n",
    "    #index_vals = df.index.values\n",
    "    #y_pred = y_pred.set_index(index_vals)\n",
    "    \n",
    "    # Other probabilities\n",
    "    #confidence_df = pd.DataFrame(confidence, columns=['1','2','3','4','5'])\n",
    "    #y_pred = pd.merge(y_pred, confidence_df, left_index=True, right_index=True)\n",
    "    \n",
    "    # Merge test flag with predictions\n",
    "    predictions = pd.merge(df_original, y_pred, left_index=True, right_index=True)\n",
    "    if 'CVs Data Score' in predictions.columns:\n",
    "        predictions = predictions.sort_values(by=['CVs Data Score'],ascending=False)\n",
    "    else:\n",
    "        predictions = predictions.sort_values(by=['WDS Score'],ascending=False)\n",
    "    #m=predictions_test[['Custom ID','Test Flag']]\n",
    "    #m=predictions_test[['Custom ID','Train Flag']]\n",
    "    #predictions = predictions.merge(m , left_on='Custom ID', right_on='Custom ID', how='left')\n",
    "    \n",
    "    # Create Secondary Prediction\n",
    "    #twomax = predictions.loc[:,['1', '2', '3', '4', '5']]\n",
    "    #twomax['Secondary Prediction'] = (twomax.rank(axis=1, ascending=False) == 2).idxmax(axis=1).astype(int)\n",
    "    #twomax['Secondary Confidence'] = twomax[twomax.rank(axis=1, ascending=False) == 3].max(axis=1)\n",
    "    #twomax = twomax.loc[:,['Secondary Prediction','Secondary Confidence']]\n",
    "    #predictions = pd.merge(predictions, twomax, left_index=True, right_index=True)\n",
    "    #predictions = predictions.rename(index=str, columns={\"1\": \"Probability of 1\", \n",
    "                                                         #\"2\": \"Probability of 2\", \n",
    "                                                         #\"3\": \"Probability of 3\",\n",
    "                                                         #\"4\": \"Probability of 4\",\n",
    "                                                         #\"5\": \"Probability of 5\"})\n",
    "    \n",
    "    # Load keywords matched\n",
    "    df_keyword = import_keyword_results(filepath_keyword,skill)\n",
    "    \n",
    "    # Merge keywords with the predictions\n",
    "    final_df = predictions.merge(df_keyword , left_on='Custom ID', right_on='Custom ID', how='left')\n",
    "    final_df['Skill_ID'] = skill_mapping.get(skill)[0]\n",
    "    final_df['Skill_Name'] = skill_mapping.get(skill)[1]\n",
    "    \n",
    "    # Generate Result Column\n",
    "    final_df['Result_Calc'] = (final_df['Skill Level'] - final_df['Predictions'])**2\n",
    "    final_df['Result'] = 'Wrong'\n",
    "    for index, row in final_df.iterrows():\n",
    "        result = ''\n",
    "        if row['Result_Calc'] < 0.5:\n",
    "            result = 'Same'\n",
    "        elif row['Result_Calc'] < 2:\n",
    "            result = 'Within 1'\n",
    "        elif row['Result_Calc'] < 5:\n",
    "            result = 'Within 2'\n",
    "        elif row['Result_Calc'] < 10:\n",
    "            result = 'Within 3'\n",
    "        elif row['Result_Calc'] < 17:\n",
    "            result = 'Within 4'\n",
    "        else:\n",
    "            result = ''\n",
    "        final_df.set_value(index,'Result',result)\n",
    "    final_df.drop(columns=['Result_Calc'],inplace=True)\n",
    "\n",
    "    return final_df, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAMESWhitfield\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due Diligence completed...\n",
      "Project Management completed...\n",
      "Real estate sell-side preparati completed...\n",
      "Formal Insolvency completed...\n",
      "Debt Options Analysis completed...\n",
      "Modelling completed...\n",
      "Impairment Testing completed...\n",
      "Advanced Excel completed...\n",
      "Purchase Price Allocation (PPA) completed...\n",
      "Working Capital & Liquidity Imp completed...\n",
      "Cash flow Forecasting completed...\n",
      "Financial Analysis Due Diligenc completed...\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "skill_list = ['Due Diligence', 'Project Management', 'Real estate sell-side preparati', 'Formal Insolvency', 'Debt Options Analysis', 'Modelling','Impairment Testing','Advanced Excel','Purchase Price Allocation (PPA)','Working Capital & Liquidity Imp','Cash flow Forecasting','Financial Analysis Due Diligenc']\n",
    "feature_cols = ['Activity Data Score','Achievement Data Score','Courses Data Score','Engagement Data Score','Feedback Data Score','BadgeData Data Score','Checklist Data Score','Learning Course Data Score','AboutMe Data Score','CVs Data Score','KCs Data Score']\n",
    "filepath_keyword = 'C:/Users/JAMESWhitfield/Documents/01 - Projects/15 - Ernst & Young/Models/Training_Data/PeopleFeatures_221118TestOnly.xls'\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "#selections = pd.DataFrame(columns = ['Skill'] + feature_cols)\n",
    "\n",
    "for skill_i in skill_list:\n",
    "    \n",
    "    filepath_WDS = 'C:/Users/JAMESWhitfield/Documents/01 - Projects/15 - Ernst & Young/Models/Training_Data/221118T/PeopleFeatures_221118TestOnly_' + skill_i + '.csv'\n",
    "    skill = skill_i\n",
    "    \n",
    "    current_df, y_pred = calculate_results(filepath_WDS,skill,feature_cols,filepath_keyword,skill_mapping)\n",
    "    \n",
    "    print(skill_i + \" completed...\")\n",
    "    \n",
    "    #print(current_selections)\n",
    "    \n",
    "    final_df = final_df.append(current_df)\n",
    "    \n",
    "    #new_row = {\n",
    "        #\"Skill\": skill_i,\n",
    "        #\"Activity Data Score\": current_selections[0],\n",
    "        #\"Achievement Data Score\": current_selections[1],\n",
    "        #\"Courses Data Score\": current_selections[2],\n",
    "        #\"Engagement Data Score\": current_selections[3],\n",
    "        #\"Feedback Data Score\": current_selections[4],\n",
    "        #\"BadgeData Data Score\": current_selections[5],\n",
    "        #\"Checklist Data Score\": current_selections[6],\n",
    "        #\"Learning Course Data Score\": current_selections[7],\n",
    "        #\"AboutMe Data Score\": current_selections[8],\n",
    "        #\"CVs Data Score\": current_selections[9],\n",
    "        #\"KCs Data Score\": current_selections[10]\n",
    "    #}\n",
    "    \n",
    "    #selections = selections.append(new_row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to a csv\n",
    "final_df.to_csv('28112018_Blind_Test_Results.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other skills and feature lists\n",
    "#skill_list = ['Due Diligence', 'Project Management', 'Real estate sell-side preparati', 'Formal Insolvency', 'Debt Options Analysis', 'Modelling','Impairment Testing','Advanced Excel','Purchase Price Allocation (PPA)','Working Capital & Liquidity Imp','Cash flow Forecasting','Financial Analysis Due Diligenc']\n",
    "#skill_list = ['Due Diligence','Real estate sell-side preparati','Formal Insolvency','Financial Analysis Due Diligenc','Stock Exchange Working Capital','Bid financial evaluation','Debt and debt-like items analys']\n",
    "#skill_list = ['Due Diligence']\n",
    "#feature_cols = ['Activity Data Score','Achievement Data Score','Courses Data Score','Engagement Minimum Data Score','Engagement Low Data Score','Engagement Medium Data Score','Engagement High Data Score','Feedback Data Score','Learning Course Data Score','AboutMe Data Score','CVs Data Score']\n",
    "#feature_cols = ['Activity Data Score','Achievement Data Score','Courses Data Score','Engagement Data Score','Feedback Data Score','Learning Course Data Score','AboutMe Data Score','CVs Data Score']\n",
    "#feature_cols = ['Activity Data Score','Courses Data Score','Engagement Data Score','Feedback Data Score','CVs Data Score']\n",
    "#feature_cols = ['WDS Score']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
